{"id": 9, "question": "Calculate the mean value of the \"Close Price\" column.", "concepts": ["Summary Statistics"], "constraints": "Use the built-in Python (numpy or pandas) to calculate the mean. Do not use any pre-built packages or libraries for mean calculation other than numpy or pandas. The calculation should be done on the whole \"Close Price\" column. Values in this column should not be rounded or changed in any way before the calculation.", "format": "@mean_close_price[mean_value], where \"mean_value\" is a float number rounded to two decimal places. This value should be between the highest and lowest \"Close Price\" given in the dataset.", "file_name": "GODREJIND.csv", "level": "easy", "answer": [["mean_close_price", "570.68"]]}
{"id": 10, "question": "Check if the \"Total Traded Quantity\" column adheres to a normal distribution.", "concepts": ["Distribution Analysis"], "constraints": "Use Shapiro-Wilk test from scipy.stats module to check for normality. In this test, the null hypothesis is that the data was drawn from a normal distribution. An alpha level of 0.05 (5%) should be taken as the significance level. If the p-value is less than the alpha level, the null hypothesis is rejected and the data does not follow a normal distribution. If the p-value is greater than the alpha level, the null hypothesis is not rejected and the data may follow a normal distribution.", "format": "@is_normal[response], where \"response\" is a string that takes the value \"yes\" if the data follows a normal distribution, and \"no\" if it does not.", "file_name": "GODREJIND.csv", "level": "easy", "answer": [["is_normal", "no"]]}
{"id": 55, "question": "What is the mean number of cases recorded across all countries and years?", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean of the column 'No. of cases'. Convert the data type of 'No. of cases' column from Object (string) to Int64 before performing calculations. Ignore those records where 'No. of cases' column value is Null or empty.", "format": "@mean_cases[mean_value] where \"mean_value\" is a positive integer.", "file_name": "estimated_numbers.csv", "level": "easy", "answer": [["mean_cases", "2081990"]]}
{"id": 56, "question": "Which country has the highest number of deaths recorded in a single year?", "concepts": ["Distribution Analysis", "Summary Statistics"], "constraints": "Calculate the maximum value in the 'No. of deaths' column. Convert the data type of 'No. of deaths' column from Object (string) to Int64 before performing calculations. Ignore those records where 'No. of deaths' column value is Null or empty. Identify the corresponding country and year for the highest number of deaths.", "format": "@max_deaths_country[country_name] @max_deaths_year[year] where \"country_name\" is a string indicating the name of the country and \"year\" is an integer indicating the year in which the maximum deaths occurred.", "file_name": "estimated_numbers.csv", "level": "easy", "answer": [["max_deaths_country", "Nigeria"], ["max_deaths_year", "2010"]]}
{"id": 57, "question": "Is there a correlation between the number of cases and the number of deaths recorded?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between number of cases and number of deaths. Convert the data types of 'No. of cases' and 'No. of deaths' column from Object (String) to Int64 before performing calculations. Do this for complete data rather than specific country or year.", "format": "@correlation_coefficient[r_value] where \"r_value\" is a number between -1 and 1, rounded to two decimal places.", "file_name": "estimated_numbers.csv", "level": "easy", "answer": [["correlation_coefficient", "0.97"]]}
{"id": 58, "question": "What is the percentage of missing values in the \"No. of cases_min\" column? How does this percentage compare to the percentage of missing values in the \"No. of deaths_max\" column?", "concepts": ["Comprehensive Data Preprocessing", "Summary Statistics"], "constraints": "Calculate the percentage of missing values for both \"No. of cases_min\" and \"No. of deaths_max\" column. Report the exact percentage values.", "format": "@percentage_cases_min[percentage], @percentage_deaths_max[percentage] where \"percentage\" is a number between 0 and 100, rounded to two decimal places.", "file_name": "estimated_numbers.csv", "level": "easy", "answer": [["percentage_cases_min", "36.45"], ["percentage_deaths_max", "38.79"]]}
{"id": 59, "question": "Among the countries in the \"Americas\" region, which country has the highest average number of cases recorded over the years?", "concepts": ["Distribution Analysis", "Summary Statistics", "Feature Engineering"], "constraints": "Calculate the average of \"No. of cases\" for each country in the \"Americas\" region and report the country with the highest average number of cases. Count only complete years, i.e., exclude years with missing data.", "format": "@country_name[country] where \"country\" is a string representing the name of the country with the highest average number of cases.", "file_name": "estimated_numbers.csv", "level": "easy", "answer": [["country_name", "Congo"]]}
{"id": 114, "question": "Which country has the highest happiness score?", "concepts": ["Summary Statistics"], "constraints": "Find the country with the highest happiness score in the dataset. If two or more countries have the same highest happiness score, return all of them.", "format": "@country_with_highest_score[country_name]", "file_name": "2015.csv", "level": "easy", "answer": [["country_with_highest_score", "Switzerland"]]}
{"id": 123, "question": "Which country has the highest average number of daily vaccinations per million people?", "concepts": ["Summary Statistics", "Distribution Analysis"], "constraints": "{\nBased on the current available data without null values in the column of daily vaccinations per million people.\nNo tie of the maximum value is allowed. In case of a tie, consider the country with the first appeared maximum value.\n}", "format": "{\n@country_with_highest_average_daily_vaccinations[country_name]\nWhere \"country_name\" is a string.", "file_name": "country_vaccinations.csv", "level": "easy", "answer": [["country_with_highest_average_daily_vaccinations", "Gibraltar"]]}
{"id": 207, "question": "1. Which column(s) contain missing values in the dataset?", "concepts": ["Comprehensive Data Preprocessing"], "constraints": "This question requires a straightforward identification of columns with missing values in the dataset. Only count the missing values in columns where the data type is 'object' (i.e., strings). Do not include columns of other data types and consider a \"missing value\" as one that is recorded as 'NaN', 'na', 'null', or an empty string in the dataset.", "format": "@missing_columns_in_object_type[missing_column1, missing_column2,â€¦] whereby 'missing_column1', 'missing_column2', etc. are string names of the columns with missing values. The answer should not contain any duplicates and should be sorted alphabetically for easy checking.", "file_name": "fb_articles_20180822_20180829_df.csv", "level": "easy", "answer": [["missing_columns_in_object_type", "author, urlToImage"]]}
{"id": 208, "question": "2. Calculate the mean and standard deviation of the \"compound\" sentiment score column.", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean and standard deviation of the 'compound' sentiment score using standard statistical methods. Please use a standard approach and do not use any approximations or assumptions. Note that the 'compound' column contains no missing values according to the scenario information.", "format": "@compound_mean[mean_value]\\n@compound_std[std_value] where 'mean_value' and 'std_value' should be rounded to three decimal places.", "file_name": "fb_articles_20180822_20180829_df.csv", "level": "easy", "answer": [["compound_mean", "0.141"], ["compound_std", "0.899"]]}
{"id": 255, "question": "Calculate the mean and standard deviation of the gross domestic product per capita in the year 2007 for all countries in the dataset. Round your answers to 2 decimal places.", "concepts": ["Summary Statistics"], "constraints": "Perform arithmetic mean and standard deviation calculations on the 'gdpPercap_2007' column of the dataset. Round your answer to two decimal places. Do not use modes, medians, or any other form of average.", "format": "@mean_gdp2007[float], @standard_deviation_gdp2007[float] where each float is a positive number rounded to two decimal places.", "file_name": "gapminder_gdp_asia.csv", "level": "easy", "answer": [["standard_deviation_gdp2007", "14154.94"], ["mean_gdp2007", "12473.03"]]}
{"id": 278, "question": "Are there any outliers in the Agri column of the dataset? If yes, how would you detect them using Z-scores?", "concepts": ["Outlier Detection"], "constraints": "Calculate the Z-scores for the Agri column values. Any data point that has a Z-score greater than 3 or less than -3 should be considered as an outlier.", "format": "@outliers_count[outliers_value] where \"outliers_value\" is a non-negative integer representing the count of outliers detected based on the Z-score calculation.", "file_name": "veracruz 2016.csv", "level": "easy", "answer": [["outliers_count", "0"]]}
{"id": 349, "question": "Calculate the mean age of the passengers.", "concepts": ["Summary Statistics"], "constraints": "The mean should be calculated on the full 'Age' column with no filtering. Use the default parameter values for pandas.DataFrame.mean method; in particular, ignore NA/null values and compute the arithmetic mean along the specified axis.", "format": "@mean_age[mean_age] where \"mean_age\" is a floating point number representing the calculated mean age, rounded to two decimal places.", "file_name": "test_x.csv", "level": "easy", "answer": [["mean_age", "1.1"]]}
{"id": 350, "question": "Check if the Fare column follows a normal distribution.", "concepts": ["Distribution Analysis"], "constraints": "Perform a Shapiro-Wilk test for normality on the 'Fare' column. Use a significance level (alpha) of 0.05 to determine if the 'Fare' column is normally distributed. The 'Fare' column is considered to be normally distributed if the p-value from the Shapiro-Wilk test is greater than 0.05.", "format": "@is_normal[is_normal] where \"is_normal\" is a boolean value: True means the 'Fare' column follows a normal distribution; False means it does not follow a normal distribution.", "file_name": "test_x.csv", "level": "easy", "answer": [["is_normal", "False"]]}
{"id": 351, "question": "Determine the correlation coefficient between Age and Fare.", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient between 'Age' and 'Fare'. Use pandas.DataFrame.corr method with the 'pearson' method. Ignore NA/null values.", "format": "@correlation_coefficient[correlation_coefficient] where \"correlation_coefficient\" is a floating point number representing the calculated correlation coefficient, rounded to two decimal places.", "file_name": "test_x.csv", "level": "easy", "answer": [["correlation_coefficient", "0.32"]]}
{"id": 352, "question": "Identify any outliers in the Fare column using the Z-score method.", "concepts": ["Outlier Detection"], "constraints": "Calculate the Z-score for each value in the Fare column. \nConsider a value to be an outlier if its Z-score is greater than 3 or less than -3.\nReturn the list of outlier values sorted in ascending order.", "format": "@fare_outliers[outliers_list]\nwhere \"outliers_list\" is a list of integers sorted in ascending order.", "file_name": "test_x.csv", "level": "easy", "answer": [["fare_outliers", ""]]}
{"id": 354, "question": "Create a new feature \"FamilySize\" by summing the IsAlone column with the number of siblings/spouses and number of parents/children on board.", "concepts": ["Feature Engineering"], "constraints": "Assume each passenger has at least one sibling/spouse and one parent/child on board, therefore, FamilySize = IsAlone + 1 (for sibling or spouse) + 1 (for parent or child).\nCompute the average FamilySize and round to one decimal place.", "format": "@average_familysize[avg_family_size]\nwhere \"avg_family_size\" is a number rounded to one decimal place.", "file_name": "test_x.csv", "level": "easy", "answer": [["average_familysize", "2.6"]]}
{"id": 409, "question": "How many missing values are there in the \"Cabin\" column?", "concepts": ["Comprehensive Data Preprocessing"], "constraints": "Count the number of missing values in the 'Cabin' column in the dataset. Treat null values as missing values.", "format": "@missing_values[missing_values] where \"missing_values\" is an integer.", "file_name": "titanic_train.csv", "level": "easy", "answer": [["missing_values", "687"]]}
{"id": 472, "question": "What is the mean value of the \"Value\" column?", "concepts": ["Summary Statistics"], "constraints": "Ignore all the null values in the \"Value\" column.\nRound your final answer to two decimal places.", "format": "@mean_value[number]\nwhere \"number\" is a floating-point number rounded to two decimal places.", "file_name": "oecd_education_spending.csv", "level": "easy", "answer": [["mean_value", "2.58"]]}
{"id": 473, "question": "Are there any outliers in the \"Value\" column? If yes, how many and what are their locations (row numbers)?", "concepts": ["Outlier Detection"], "constraints": "Use the IQR method to detect outliers. Define an outlier as a data point that falls below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\nReturn the list of row numbers (starting from 0) for those outliers in ascending order. If there are no outliers, return an empty list.\nIgnore the null values in the \"Value\" column.", "format": "@outliers[list_of_numbers]\nwhere \"list_of_numbers\" is a list of integers.", "file_name": "oecd_education_spending.csv", "level": "easy", "answer": [["outliers", ""]]}
{"id": 474, "question": "Is there a correlation between the \"Value\" column and the \"TIME\" column? If yes, what is the correlation coefficient?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient.\nIgnore all the pairs that either \"Value\" or \"TIME\" is null.\nRound your final answer to two decimal places.", "format": "@correlation_coefficient[number]\nwhere \"number\" is a floating-point number between -1 and 1, rounded to two decimal places.", "file_name": "oecd_education_spending.csv", "level": "easy", "answer": [["correlation_coefficient", "0.02"]]}
{"id": 490, "question": "What is the mean percentage of graduates in the field of Engineering?", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean value using all available data points in the field of Engineering. Round the mean value to two decimal places.", "format": "@mean_value[number] where \"number\" is a number between 0 and 100, representing the mean value, rounded to two decimal places.", "file_name": "percent-bachelors-degrees-women-usa.csv", "level": "easy", "answer": [["mean_value", "12.89"]]}
{"id": 492, "question": "Which field has the highest percentage of graduates in the year 2010?", "concepts": ["Summary Statistics"], "constraints": "Compare the last available data points (year 2010) for all fields within the dataset. If fields share the maximum percentage value, return all those fields separated by commas.", "format": "@fields[field_names] where \"field_names\" is string(s) separated by commas, mentioning field(s) with the highest percentage of graduates in the year 2010.", "file_name": "percent-bachelors-degrees-women-usa.csv", "level": "easy", "answer": [["fields", "Health Professions"]]}
{"id": 551, "question": "What is the mean of the DBH_CM column?", "concepts": ["Summary Statistics"], "constraints": "Calculate the arithmetic mean of the 'DBH_CM' column. The answer should be rounded to the nearest hundredth. Do not consider missing values, outliers, or data error possibilities, as it was stated there are no missing values in this column and no further cleaning or preprocessing is needed for this problem.", "format": "@mean_dbh_cm[mean_value] where 'mean_value' is a float number with two decimal values.", "file_name": "tree.csv", "level": "easy", "answer": [["mean_dbh_cm", "37.96"]]}
{"id": 553, "question": "How many outliers are there in the TPH_PLT column?", "concepts": ["Outlier Detection"], "constraints": "Detect outliers in the 'TPH_PLT' column using the IQR method, where observations that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are considered outliers. Do not consider missing values, as it was stated there are no missing values in this column.", "format": "@outliers_count[count] where 'count' is a non-negative integer.", "file_name": "tree.csv", "level": "easy", "answer": [["outliers_count", "3131"]]}
{"id": 554, "question": "What is the median HT_M value for the plant species with a CON value of 1, and a PLTID of 5?", "concepts": ["Summary Statistics", "Distribution Analysis"], "constraints": "Filter the data frame first by CON value of 1, then by PLTID of 5, calculate the median HT_M value of these entries.", "format": "@median_ht_m[median_value] where \"median_value\" is a float rounded to 2 decimal places.", "file_name": "tree.csv", "level": "easy", "answer": [["median_ht_m", "nan"]]}
{"id": 555, "question": "How many unique plant species (represented by unique SPP_SYMBOL values) are there in the dataset, where each species has at least 5 observations?", "concepts": ["Feature Engineering"], "constraints": "Count unique SPP_SYMBOL values that appear at least 5 times.", "format": "@unique_species_count[species_count] where \"species_count\" is an integer.", "file_name": "tree.csv", "level": "easy", "answer": [["unique_species_count", "29"]]}
{"id": 586, "question": "Find out the total number of calls that were abandoned by the callers before being answered by an agent.", "concepts": ["Distribution Analysis"], "constraints": "Use Python's pandas DataFrame to load the CSV file. Perform the data cleaning step to ensure there're no null or NaN values for the \"num. calls abandoned\" column. Then use the sum() function on this column to get the total.", "format": "@total_abandoned_calls[integer], where integer represents the total number of calls that were abandoned by the callers before being answered by an agent.", "file_name": "20170413_000000_group_statistics.csv", "level": "easy", "answer": [["total_abandoned_calls", "9"]]}
{"id": 666, "question": "Calculate the mean and standard deviation of the MedianHouseValue column in the provided dataset.", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean and standard deviation to four decimal places using built-in Python statistical functions.", "format": "@mean_value[mean], @std_dev[std_dev] where \"mean\" and \"std_dev\" are values rounded to four decimal places.", "file_name": "my_test_01.csv", "level": "easy", "answer": [["std_dev", "1.2210"], ["mean_value", "2.1226"]]}
{"id": 683, "question": "1. What is the mean temperature recorded in the dataset?", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean temperature to two decimal places. No missing values in the temperature data.", "format": "@mean_temperature[value], where \"value\" is a number representing the mean temperature, rounded to two decimal places.", "file_name": "ravenna_250715.csv", "level": "easy", "answer": [["mean_temperature", "29.14"]]}
{"id": 710, "question": "1. What is the mean number of wins in the \"JAMES LOGAN\" column?", "concepts": ["Summary Statistics"], "constraints": "Assume all values in the \"JAMES LOGAN\" column are numeric, and convert strings to numbers if necessary. Ignore any rows where \"JAMES LOGAN\" is missing or cannot be converted to a number. Use pandas `mean()` function to calculate the mean.", "format": "@mean_wins[mean]", "file_name": "Current_Logan.csv", "level": "easy", "answer": [["mean_wins", "2.6"]]}
{"id": 715, "question": "3. What is the percentage of missing values in the \"Unnamed: 8\" column?", "concepts": ["Comprehensive Data Preprocessing"], "constraints": "The missing values are represented as NaN in pandas dataframe.", "format": "@missing_percentage[percentage], where \"percentage\" is a number between 0 and 100, representing the percentage of missing values in the column, rounded to two decimal places.", "file_name": "Current_Logan.csv", "level": "easy", "answer": [["missing_percentage", "95.12"]]}
{"id": 719, "question": "1. Calculate the mean and median of the 'mpg' column.", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean and median of the 'mpg' column without excluding any data. Round your results to two decimal places.", "format": "@mean_mpg[mean_value], @median_mpg[median_value] where 'mean_value' and 'median_value' are numbers rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "easy", "answer": [["median_mpg", "22.75"], ["mean_mpg", "23.45"]]}
{"id": 729, "question": "Does the distribution of GDP per capita adhere to a normal distribution?", "concepts": ["Distribution Analysis"], "constraints": "Use the scipy library's normaltest() function on the \"Gdppercap\" column. Consider the distribution to be normal if p-value is greater than 0.05.", "format": "@distribution_normality[distribution_type] where \"distribution_type\" is a string which is either \"normal\" if condition is met or \"not normal\" if otherwise.", "file_name": "gapminder_cleaned.csv", "level": "easy", "answer": [["distribution_normality", "not normal"]]}
{"id": 755, "question": "1. What is the mean value of the maximum temperature (TMAX_F) recorded in the dataset?", "concepts": ["Summary Statistics"], "constraints": "Calculate the mean (average) as the sum of all recorded values divided by the total number of observations.", "format": "@mean_TMAX_F[mean_temperature] where \"mean_temperature\" is a positive number rounded to two decimal places.", "file_name": "weather_data_1864.csv", "level": "easy", "answer": [["mean_TMAX_F", "56.38"]]}
{"id": 11, "question": "Calculate the correlation coefficient between the \"High Price\" column and the \"Low Price\" column.", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between \"High Price\" and \"Low Price\". Assess the significance of the correlation using a two-tailed test with a significance level (alpha) of 0.05. Report the p-value associated with the correlation test. Consider the relationship to be linear if the p-value is less than 0.05 and the absolute value of r is greater than or equal to 0.5. Consider the relationship to be nonlinear if the p-value is less than 0.05 and the absolute value of r is less than 0.5. If the p-value is greater than or equal to 0.05, report that there is no significant correlation.", "format": "@correlation_coefficient[r_value] @p_value[p_value] @relationship_type[relationship_type], where \"r_value\" is a number between -1 and 1, rounded to two decimal places. \"p_value\" is a number between 0 and 1, rounded to four decimal places. \"relationship_type\" is a string that can either be \"linear\", \"nonlinear\", or \"none\" based on the conditions specified in the constraints.", "file_name": "GODREJIND.csv", "level": "medium", "answer": [["relationship_type", "linear"], ["correlation_coefficient", "0.99"]]}
{"id": 14, "question": "Create a new feature called \"Price Range\" which represents the difference between the \"High Price\" and \"Low Price\" for each row. Calculate the mean, median, and standard deviation of this new feature.", "concepts": ["Feature Engineering", "Summary Statistics"], "constraints": "Make sure to use the correct columns for calculating the \"Price Range\". All calculations should be performed up to two decimal places.", "format": "@price_range_mean[mean]: The mean should be a single real number rounded to two decimal places. @price_range_median[median]: The median should be a single real number rounded to two decimal places. @price_range_std_dev[std_dev]: The standard deviation should be a single real number rounded to two decimal places.", "file_name": "GODREJIND.csv", "level": "medium", "answer": [["price_range_mean", "16.65"], ["price_range_std_dev", "6.72"], ["price_range_median", "15.67"]]}
{"id": 62, "question": "Are there any outliers in the \"No. of deaths_max\" column for each country? How do these outliers affect the overall distribution of recorded deaths?", "concepts": ["Outlier Detection", "Distribution Analysis"], "constraints": "Use the IQR method (1.5*IQR rule) to detect the outliers. If there are any outliers, remove them and then recalculate the mean number of deaths.", "format": "@no_of_countries_with_outliers[number], @mean_no_of_deaths_with_outliers[original_mean], @mean_no_of_deaths_without_outliers[new_mean]. The number should be an integer. The original_mean and new_mean should be float numbers rounded to two decimal places.", "file_name": "estimated_numbers.csv", "level": "medium", "answer": [["mean_no_of_deaths_with_outliers", "10149.43"], ["mean_no_of_deaths_without_outliers", "5949.08"]]}
{"id": 116, "question": "Are there any outliers in the happiness scores of countries? If so, which countries are considered outliers?", "concepts": ["Outlier Detection"], "constraints": "Outliers should be determined by the Z-score method. If a country has a Z score greater than 3 or less than -3, it is considered an outlier. The calculation should be done using the population standard deviation formula.", "format": "@outlier_countries[country1,country2,...] where \"country1,country2,...\": a list of countries that are outliers based on their happiness scores.", "file_name": "2015.csv", "level": "medium", "answer": [["outlier_countries", ""]]}
{"id": 117, "question": "Which variable has the strongest correlation with the happiness scores among countries? Is this correlation positive or negative?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) between the happiness score and all other numerical variables in the dataset. The variable which has the highest magnitude of r (ignoring the sign) is the one with the strongest correlation.", "format": "@strongest_correlation_variable[variable_name] where \"variable_name\": the column name of the variable with the strongest correlation. @correlation_type[positive/negative] where \"positive/negative\": if the correlation is positive or negative based on the sign of the correlation coefficient.", "file_name": "2015.csv", "level": "medium", "answer": [["correlation_type", "negative"], ["strongest_correlation_variable", "Happiness Rank"]]}
{"id": 209, "question": "3. Is there any correlation between the \"neg\" and \"pos\" sentiment score columns? If so, what is the correlation coefficient?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between 'neg' and 'pos' sentiment scores. If the Pearson correlation coefficient (absolute value) is close to 1, it means that there exists a strong correlation. If it is close to 0, it means that there exists a weak or no correlation. If the coefficient is positive, the correlation is positive; if negative, the correlation is negative.", "format": "@correlation_coefficient[r_value] where 'r_value' is a number between -1 and 1, rounded to two decimal places.", "file_name": "fb_articles_20180822_20180829_df.csv", "level": "medium", "answer": [["correlation_coefficient", "-0.24"]]}
{"id": 252, "question": "Determine which country's gross domestic product per capita in the year 1992 had the highest skewness among all countries in the dataset.", "concepts": ["Distribution Analysis"], "constraints": "Use Python's SciPy library to calculate the skewness of each country's gross domestic product per capita in 1992. Skewness should be calculated with Fisherâ€™s definition, i.e. the one that's adjusted for the normal distribution.", "format": "@highest_skewness_country[country_name] where \"country_name\" is a string", "file_name": "gapminder_gdp_asia.csv", "level": "medium", "answer": [["highest_skewness_country", "Afghanistan"]]}
{"id": 254, "question": "Identify any outliers in the gross domestic product per capita data for the year 1982 for all countries. Define an outlier as any data point that falls more than 1.5 times the interquartile range (IQR) below the first quartile or above the third quartile. Report the country or countries which their gdpPercap_1982 values are identified as outliers.", "concepts": ["Outlier Detection"], "constraints": "Use the interquartile range (IQR) rule for outlier detection: a data point is considered an outlier if it falls more than 1.5*IQR below the first quartile (Q1) or above the third quartile (Q3). Don't use any other outlier detection methods or parameters.", "format": "@outlier_countries[list_of_strings] where each string in list_of_strings is the name of a country that is an outlier according to the IQR rule.", "file_name": "gapminder_gdp_asia.csv", "level": "medium", "answer": [["outlier_countries", "Kuwait, Saudi Arabia"]]}
{"id": 277, "question": "Is there any correlation between the MedInd and LarInd columns in the given dataset? If yes, what is the correlation coefficient?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson's correlation coefficient (r), a statistical measure that calculates the strength of the relationship between the relative movements of two variables, between the MedInd and LarInd columns. The Pearson's correlation coefficient should be rounded to 4 decimal places.", "format": "@correlation_coefficient[correlation_value] where \"correlation_value\" is a signed numeric value between -1 and 1, rounded to 4 decimal places.", "file_name": "veracruz 2016.csv", "level": "medium", "answer": [["correlation_coefficient", "0.7366"]]}
{"id": 408, "question": "Is there a correlation between the fare paid by the passenger and their age? If so, is it a linear or nonlinear correlation?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between 'Fare' and 'Age'.\nAssess the significance of the correlation using a two-tailed test with a significance level (alpha) of 0.05.\nReport the p-value associated with the correlation test.\nConsider the relationship to be linear if the p-value is less than 0.05 and the absolute value of r is greater than or equal to 0.5.\nConsider the relationship to be nonlinear if the p-value is less than 0.05 and the absolute value of r is less than 0.5.\nIf the p-value is greater than or equal to 0.05, report that there is no significant correlation.\nIgnore the null values in 'Age' while calculating the correlation.", "format": "@correlation_coefficient[r_value]\n@p_value[p_value]\n@relationship_type[relationship_type] \nwhere \"r_value\" is a number between -1 and 1, rounded to two decimal places.\nwhere \"p_value\" is a number between 0 and 1, rounded to four decimal places.\nwhere \"relationship_type\" is a string that can either be \"linear\", \"nonlinear\", or \"none\" based on the conditions specified in the constraints.", "file_name": "titanic_train.csv", "level": "medium", "answer": [["correlation_coefficient", "0.10"], ["relationship_type", "nonlinear"], ["p_value", "0.0102"]]}
{"id": 410, "question": "What is the distribution of ages among the male passengers who did not survive? Is it significantly different from the distribution of ages among the female passengers who did not survive?", "concepts": ["Distribution Analysis"], "constraints": "Calculating the distribution of ages should use a Kernel Density Estimation (KDE) method. Perform a two-sample Kolmogorov-Smirnov test to compare the distributions. Use a significance level (alpha) of 0.05. If the p-value is less than 0.05, conclude the distributions are significantly different. If the p-value is greater than or equal to 0.05, conclude the distributions are not significantly different.", "format": "@is_significantly_different[answer] where \"answer\" is a boolean indicating the result of the test. For example, if the distributions are significantly different, the answer should be \"True\". If not, the answer should be \"False\".", "file_name": "titanic_train.csv", "level": "medium", "answer": [["is_significantly_different", "True"]]}
{"id": 411, "question": "Are there any outliers in the fare paid by the passengers? If so, how many outliers are there and what is their range?", "concepts": ["Outlier Detection"], "constraints": "An outlier is identified based on the IQR method. An outlier is defined as a point that falls outside 1.5 times the IQR above the third quartile or below the first quartile.", "format": "@outlier_count[answer1] @outlier_range_low[answer2] @outlier_range_high[answer3] where \"answer1\" is the number of outliers, \"answer2\" is the lowest value among outliers and \"answer3\" is the highest value among outliers. All results should be rounded to 2 decimal places.", "file_name": "titanic_train.csv", "level": "medium", "answer": [["outlier_range_high", "512.33"], ["outlier_count", "116"]]}
{"id": 412, "question": "Create a new feature called \"FamilySize\" by adding the \"SibSp\" and \"Parch\" columns together. What is the mean \"FamilySize\" for passengers who survived versus passengers who did not survive?", "concepts": ["Feature Engineering"], "constraints": "Calculate the mean of \"FamilySize\" separately for the passengers who survived and the passengers who did not survive. \"FamilySize\" should be an integer value. The mean should be calculated rounding up to two decimal places.", "format": "@mean_familysize_survived[answer1] @mean_familysize_did_not_survive[answer2] where \"answer1\" is the mean \"FamilySize\" for passengers who survived and \"answer2\" is the mean \"FamilySize\" for passengers who did not survive. Both results should be rounded to 2 decimal places.", "file_name": "titanic_train.csv", "level": "medium", "answer": [["mean_familysize_survived", "0.94"], ["mean_familysize_did_not_survive", "0.88"]]}
{"id": 414, "question": "What is the average age of passengers in each ticket class (Pclass)?", "concepts": ["Summary Statistics", "Comprehensive Data Preprocessing"], "constraints": "Calculate the average (mean) age of the passengers in each class separately (Pclass = 1, Pclass = 2, Pclass = 3).\nIgnore the rows with missing age.\nRound the average age to two decimal places.", "format": "@first_class_average_age[average_age_1]\n@second_class_average_age[average_age_2]\n@third_class_average_age[average_age_3]\nwhere \"average_age_1\" is the average age of the first-class passengers, rounded to two decimal places.\nwhere \"average_age_2\" is the average age of the second-class passengers, rounded to two decimal places.\nwhere \"average_age_3\" is the average age of the third-class passengers, rounded to two decimal places.", "file_name": "titanic_train.csv", "level": "medium", "answer": [["first_class_average_age", "38.23"], ["second_class_average_age", "29.88"], ["third_class_average_age", "25.14"]]}
{"id": 552, "question": "Are the HT_M column and the BA_M2 column correlated?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between the 'HT_M' and 'BA_M2' columns. The answer should be rounded to the third decimal place. Consider the relationship to be linear if the absolute value of r is greater than or equal to 0.5. Consider the relationship to be non-linear if the absolute value of r is less than 0.5.", "format": "@correlation_coefficient[r_value] @relationship_type[relationship_type] where 'r_value' is a float number between -1 and 1 with three decimal places and 'relationship_type' is a string that is either 'linear', 'nonlinear'.", "file_name": "tree.csv", "level": "medium", "answer": [["relationship_type", "linear"], ["correlation_coefficient", "0.806"]]}
{"id": 587, "question": "Examine the correlation between the average number of agents talking and the average waiting time for callers.", "concepts": ["Correlation Analysis"], "constraints": "Transform the average waiting time from 'HH:MM:SS' string format to seconds (integer type). Then use the Pearson's method to calculate the correlation coefficient between the average number of agents talking and the transformed average waiting time. The result should be rounded to three decimal places.", "format": "@correlation_coefficient[float], where float is a number between -1 and 1 that measures the correlation between the average number of agents talking and the average waiting time for callers. The number should be rounded to three decimal places.", "file_name": "20170413_000000_group_statistics.csv", "level": "medium", "answer": [["correlation_coefficient", "0.639"]]}
{"id": 588, "question": "Are there any outliers in the average wait time for callers before being answered by an agent? If so, how many outliers are there?", "concepts": ["Outlier Detection"], "constraints": "Detect the outliers using the Z-score method. Consider any data point with an absolute Z-score value greater than 3 as an outlier.", "format": "@num_of_outliers[number_of_outliers] where \"number_of_outliers\" is a non-negative integer value representing the number of outliers detected based on the Z-score method.", "file_name": "20170413_000000_group_statistics.csv", "level": "medium", "answer": [["num_of_outliers", "2"]]}
{"id": 589, "question": "Can we generate a new feature representing the call abandonment rate? If so, what is the call abandonment rate for the timestamp \"20170413_080000\"?", "concepts": ["Feature Engineering"], "constraints": "Calculate the call abandonment rate for a specific timestamp as the total number of calls abandoned divided by the total number of calls made during that time. Express the result as a percentage.", "format": "@abandonment_rate[abandonment_rate_%] where \"abandonment_rate_%\" is a positive real value between 0 and 100, rounded to two decimal places, representing the abandonment rate at the specified timestamp.", "file_name": "20170413_000000_group_statistics.csv", "level": "medium", "answer": [["abandonment_rate", "6.25"]]}
{"id": 667, "question": "Check if the MedInc column adheres to a normal distribution in the provided dataset.", "concepts": ["Distribution Analysis"], "constraints": "Perform a Shapiro-Wilk test at a significance level (alpha) of 0.05 to assess normality of the MedInc column. Report the p-value associated with the normality test. If the p-value is greater than 0.05, state that the data is normally distributed. If the p-value is less than or equal to 0.05, state that the data is not normally distributed.", "format": "@p_value[p_value], @distribution_type[distribution_type] where \"p_value\" is a number between 0 and 1, rounded to four decimal places, and \"distribution_type\" is a string that can either be \"normal\" or \"not normal\" based on the conditions specified in the constraints.", "file_name": "my_test_01.csv", "level": "medium", "answer": [["distribution_type", "not normal"]]}
{"id": 668, "question": "Calculate the correlation coefficient between the HouseAge and MedianHouseValue columns in the provided dataset.", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient to assess the strength and direction of the linear relationship between HouseAge and MedianHouseValue. Report the p-value associated with the correlation test with a significance level of 0.05. Indicate whether or not there is a significant correlation based on the p-value.", "format": "@correlation_coefficient[r_value], @p_value[p_value], @significant_correlation[significant_correlation] where \"r_value\" is a number between -1 and 1, rounded to two decimal places; \"p_value\" is a number between 0 and 1, rounded to four decimal places; \"significant_correlation\" is a boolean value indicating whether there is a significant correlation (true) or not (false) based on the conditions specified in the constraints.", "file_name": "my_test_01.csv", "level": "medium", "answer": [["correlation_coefficient", "0.13"], ["p_value", "0.0324"], ["significant_correlation", "true"]]}
{"id": 684, "question": "2. Does the humidity level in the dataset adhere to a normal distribution?", "concepts": ["Distribution Analysis"], "constraints": "Use the Shapiro-Wilk test with a significance level (alpha) of 0.05 to determine if the distribution of the humidity level adheres to a normal distribution. Report the p-value associated with the test. If the p-value is greater than 0.05, it can be considered as normally distributed; otherwise, it is not.", "format": "@shapiro_p_value[value] @distribution_type[type], where \"value\" is a number representing the p-value from the Shapiro-Wilk test, rounded to four decimal places, and \"type\" is a string that can either be \"normal\" or \"not normal\" based on the p-value.", "file_name": "ravenna_250715.csv", "level": "medium", "answer": [["distribution_type", "normal"], ["shapiro_p_value", "0.9166"]]}
{"id": 688, "question": "3. Using feature engineering, create a new feature called \"time_of_day\" based on the \"dt\" column. The \"time_of_day\" feature should categorize the timestamp into morning (6:00 to 11:59), afternoon (12:00 to 17:59), evening (18:00 to 23:59), and night (0:00 to 5:59) (included). Provide the count of each category in the \"time_of_day\" column.", "concepts": ["Feature Engineering"], "constraints": "For each time of the day, include the first minute of each category and exclude the first minute of the next category. If there's multiple entry which belongs to the same minute, account them all into the corresponding category.", "format": "@morning[integer], @afternoon[integer], @evening[integer], @night[integer]", "file_name": "ravenna_250715.csv", "level": "medium", "answer": [["morning", "6"], ["afternoon", "6"]]}
{"id": 716, "question": "1. Perform data preprocessing by dropping the rows where the \"Wins\" in the \"JAMES LOGAN\" column is missing, and calculate the mean and standard deviation of the remaining \"Wins\" values.", "concepts": ["Summary Statistics", "Comprehensive Data Preprocessing"], "constraints": "{\nExclude rows where \"Wins\" is missing or is a non-numeric value.\nConvert \"Wins\" to numeric values before calculations.\nCompute the mean and standard deviation to two decimal places.\n}", "format": "{\n@mean_wins[mean_wins]\n@stddev_wins[stddev_wins]\nwhere \"mean_wins\" and \"stddev_wins\" are numeric values rounded off to two decimal places.", "file_name": "Current_Logan.csv", "level": "medium", "answer": [["stddev_wins", "1.17"]]}
{"id": 721, "question": "3. Find the correlation coefficient between the 'mpg' and 'weight' columns.", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between mpg and weight. Round r to two decimal places.", "format": "@correlation_coefficient[r_value] where 'r_value' is a number between -1 and 1, rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "medium", "answer": [["correlation_coefficient", "-0.83"]]}
{"id": 730, "question": "Is there a correlation between population and GDP per capita for the recorded years and countries in the dataset?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (pearsonâ€™s r) between \"Pop\" and \"Gdppercap\" columns. Use the scipy library's pearsonr() function and consider the correlation to be significant if p-value is less than 0.05.", "format": "@correlation_coefficient[r_value]\n@p_value[p_value]\nwhere \"r_value\" is a number between -1 and 1, rounded to two decimal places.\nwhere \"p_value\" is a number between 0 and 1, rounded to four decimal places.", "file_name": "gapminder_cleaned.csv", "level": "medium", "answer": [["p_value", "0.2909"], ["correlation_coefficient", "-0.03"]]}
{"id": 756, "question": "2. Is there a correlation between the maximum temperature (TMAX_F) and the observation values (obs_value)? If yes, what is the correlation coefficient?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient(r) to assess the strength and direction of the linear relationship between TMAX_F and obs_value. Conduct the test at a significance level (alpha) of 0.05. If the p-value is less than 0.05, report the p-value and r-value. If the p-value is greater than or equal to 0.05, report that there is no significant correlation.", "format": "@correlation_coefficient[r_value] @p_value[p_value] where \"r_value\" is a number between -1 and 1, rounded to two decimal places; \"p_value\" is a number between 0 and 1, rounded to four decimal places. If there is no significant correlation, please simply output @correlation_status[\"No significant correlation\"]", "file_name": "weather_data_1864.csv", "level": "medium", "answer": [["correlation_coefficient", "1.00"], ["p_value", "0.0000"]]}
{"id": 757, "question": "3. Are there any outliers in the observation values (obs_value) column? If yes, how many outliers are there using the interquartile range method?", "concepts": ["Outlier Detection"], "constraints": "Calculate the interquartile range (IQR) for obs_value. Any value that falls below Q1 - 1.5*IQR or above Q3 + 1.5*IQR is considered an outlier. Count the number of outliers according to this method.", "format": "@outlier_count[total_outlier] where \"total_outlier\" is an integer representing the number of outliers. If there are no outliers, output @outlier_status[\"No Outliers Detected\"]", "file_name": "weather_data_1864.csv", "level": "medium", "answer": [["outlier_count", "25"]]}
{"id": 760, "question": "6. For each station, are there any missing values in the observation values (obs_value)? If yes, which station has the most missing values and how many missing values does it have?", "concepts": ["Comprehensive Data Preprocessing"], "constraints": "In your analysis:\n- Assume that missing values are represented as \"NaN\".\n- Calculate the number of missing values for each station.", "format": "@most_missing_station_name[\"station_name\"]\n@most_missing_station_count[num_missing_obs]\n\nwhere \"station_name\" is a string representing the name of the station with the most missing observation value.\nwhere \"num_missing_obs\" is a number greater than or equal to 0, representing the number of missing observation values for the station with the most missing values.", "file_name": "weather_data_1864.csv", "level": "medium", "answer": [["most_missing_station_name", "\"AGE00135039\""], ["most_missing_station_count", "0"]]}
{"id": 118, "question": "Is there a linear relationship between the GDP per capita and the life expectancy score in the dataset? Conduct linear regression and use the resulting coefficient of determination (R-squared) to evaluate the model's goodness of fit.", "concepts": ["Correlation Analysis", "Machine Learning"], "constraints": "Calculate the coefficient of determination (R-squared) for the given relationship. If R-squared is equal to or greater than 0.7, consider the model a good fit. Else, consider it a poor fit.", "format": "@coefficient_determination[R_square], @model_fit[model_fit], where \"R_square\" is the value of the coefficient of determination rounded to two decimal places and \"model_fit\" is a string that is either \"good fit\" or \"poor fit\" based on the calculated R-squared value.", "file_name": "2015.csv", "level": "hard", "answer": [["coefficient_determination", "0.67"], ["model_fit", "poor fit"]]}
{"id": 124, "question": "Is there a significant difference in the total number of vaccinations administered per hundred people between countries that use different vaccines?", "concepts": ["Summary Statistics", "Correlation Analysis"], "constraints": "{\nOnly consider countries using Pfizer/BioNTech, Moderna, Oxford/AstraZeneca, and Johnson&Johnson/Janssen. \nThe country must have data without null values in the column of total vaccinations per hundred people.\nUse One-Way Analysis of Variance (ANOVA) to test if there's significant difference among different vaccine groups. \nConsider the differences among vaccine groups to be significant if the p-value is less than 0.05.\n}", "format": "{\n@significance_of_difference[significance]\n@p_value[p_value]\nWhere \"significance\" is a string that can either be \"yes\" or \"no\" based on the conditions specified in the constraints.\nWhere \"p_value\" is a number between 0 and 1, rounded to four decimal places.", "file_name": "country_vaccinations.csv", "level": "hard", "answer": [["significance_of_difference", "no"]]}
{"id": 125, "question": "Can we predict the number of people fully vaccinated per hundred people based on the total number of vaccinations administered and the number of people vaccinated per hundred people?", "concepts": ["Correlation Analysis", "Machine Learning"], "constraints": "{\nPerform a multiple linear regression analysis using the total number of vaccinations administered and the number of people vaccinated per hundred people as predictors.\nThe dependent variable is the number of people fully vaccinated per hundred people.\nOnly consider data entries without null values in the three mentioned columns.\nUse a significance level (alpha) of 0.05 for the predictors.\nConsider the predictors to be significant if the p-value is less than 0.05.\nCalculate the R-squared value of the model.\n}", "format": "{\n@significant_predictor[predictor_1,predictor_2]\n@r_squared[r_squared_value]\nWhere \"predictor_1,predictor_2\" can be \"yes,yes\", \"yes,no\", \"no,yes\", or \"no,no\" based on the p-values of the predictors.\nWhere \"r_squared_value\" is a number between 0 and 1, rounded to four decimal places.", "file_name": "country_vaccinations.csv", "level": "hard", "answer": [["significant_predictor", "yes,yes"], ["r_squared", "0.6059"]]}
{"id": 210, "question": "1. Identify and remove any outliers in the \"neg\" sentiment score column using the Z-score method, where Z is defined as (value - mean) / standard deviation. Assume a data point to be an outlier if its Z-score is greater than 3 or less than -3. After removing outliers, calculate the new mean and standard deviation for the \"neg\" sentiment score column.", "concepts": ["Outlier Detection", "Summary Statistics"], "constraints": "Z-score is calculated with its general mathematical formula (value - mean) / standard deviation. Consider a data point as an outlier if its Z-score is greater than 3 or less than -3. Do this for the \"neg\" sentiment score column only.", "format": "@mean_neg[mean]\\n@std_dev_neg[std_dev] where \"mean\" and \"std_dev\" are floating-point numbers rounded to two decimal places. Additionally, \"mean\" and \"std_dev\" should be greater than 0 and less than 1 as they mimic sentiment scores.", "file_name": "fb_articles_20180822_20180829_df.csv", "level": "hard", "answer": [["mean_neg", "0.07"], ["std_dev_neg", "0.04"]]}
{"id": 214, "question": "2. Perform a correlation analysis between the sentiment scores (\"neg\", \"neu\", \"pos\") and the article length (\"text\" column non-space character count) for articles published by the source \"ABC News\". Identify any significant correlations between the variables and provide a brief explanation of the findings.", "concepts": ["Correlation Analysis", "Feature Engineering"], "constraints": "{\n- Use Pearson correlation for the correlation analysis.\n- Assess the strength of the correlation between each pair of variables. Consider correlations to be weak if |r| < 0.3, moderate if 0.3 <= |r| < 0.5, and strong if |r| >= 0.5. \n}", "format": "{\n@neg_length_corr[neg_length_correlation]\n@neu_length_corr[neu_length_correlation]\n@pos_length_corr[pos_length_correlation]\nwhere \"neg_length_correlation\", \"neu_length_correlation\" and \"pos_length_correlation\" are decimal numbers between -1 and 1 (rounded to 2 decimal places) representing the strength of the correlation between the sentiment scores and the article length.", "file_name": "fb_articles_20180822_20180829_df.csv", "level": "hard", "answer": [["pos_length_corr", "-0.35"], ["neu_length_corr", "0.42"]]}
{"id": 282, "question": "Perform correlation analysis on the given dataset to determine if there is any relationship between the Agri and Residential columns. Additionally, explore the distribution of the Agri column and identify any outliers using z-score as the outlier detection method. Treat any value which has z-score above 3 as an outlier.", "concepts": ["Correlation Analysis", "Distribution Analysis", "Outlier Detection"], "constraints": "Calculate the Pearson correlation coefficient to assess the linear relationship between Agri and Residential columns. Treat a value as an outlier if the z-score is above 3. Do not consider any other method for outlier detection. Use scipy's pearsonr method for correlation calculation.", "format": "@correlation_coefficient[r_value]\\n@number_of_outliers[number_of_outliers]\\n where \"r_value\" is a number between -1 and 1, rounded to two decimal places. \"number_of_outliers\" is an integer representing the number of outliers found in the Agri column.", "file_name": "veracruz 2016.csv", "level": "hard", "answer": [["number_of_outliers", "0"], ["correlation_coefficient", "-0.17"]]}
{"id": 355, "question": "Perform a linear regression analysis to predict fare based on age and passenger class.", "concepts": ["Correlation Analysis", "Machine Learning"], "constraints": "{\nUse the simple linear regression model where Fare is the dependent variable and Age and Pclass are the independent variables.\nConsider the relationship to be significant if the p-value is less than 0.05 for both variables (Age and Pclass).\nIf the p-value is greater than or equal to 0.05 for either variable, report that there is no significant relationship.\n}", "format": "{\n@coef_age[coef_age]\n@coef_pclass[coef_pclass]\n@relationship_age[relationship_age]\n@relationship_pclass[relationship_pclass]\nwhere \"coef_age\" and \"coef_pclass\" are the regression coefficients for the variables Age and Pclass respectively, rounded to two decimal places.\nwhere \"relationship_age\" and \"relationship_pclass\" are strings that can either be \"significant\" or \"not significant\" based on the conditions specified in the constraints.", "file_name": "test_x.csv", "level": "hard", "answer": [["relationship_age", "not significant"], ["relationship_pclass", "significant"], ["coef_pclass", "-0.98"], ["coef_age", "-0.05"]]}
{"id": 413, "question": "Is there a correlation between the ticket class (Pclass) and the fare paid by the passengers that embarked from Cherbourg (Embarked = 'C')?", "concepts": ["Correlation Analysis", "Comprehensive Data Preprocessing"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between Pclass and Fare for passengers who embarked from Cherbourg.\nAssess the significance of the correlation using a two-tailed test with a significance level (alpha) of 0.01.\nReport the p-value associated with the correlation test.\nConsider the relationship to be significant if the p-value is less than 0.01.\nIf the p-value is greater than or equal to 0.01, report that there is no significant correlation.", "format": "@correlation_coefficient[r_value]\n@p_value[p_value]\n@relationship_significance[significance]\nwhere \"r_value\" is a number between -1 and 1, rounded to two decimal places.\nwhere \"p_value\" is a number between 0 and 1, rounded to four decimal places.\nwhere \"significance\" is a string that can either be \"significant\" or \"not significant\" based on the conditions specified in the constraints.", "file_name": "titanic_train.csv", "level": "hard", "answer": [["correlation_coefficient", "-0.53"], ["relationship_significance", "significant"], ["p_value", "0.0000"]]}
{"id": 415, "question": "What is the distribution of fare paid by male passengers who survived? Are there any significant differences in the fare paid by male passengers who survived compared to male passengers who did not survive?", "concepts": ["Distribution Analysis", "Comprehensive Data Preprocessing"], "constraints": "Calculate the mean and standard deviation of fares paid by male passengers who survived and did not survive separately.\nConduct an independent sample t-test to compare the means of these two groups.\nUse a significance level of 0.05.\nReport whether there is a significant difference in the means based on the p-value of the test.", "format": "@survived_fare_mean[mean_survived]\n@survived_fare_std[std_survived]\n@not_survived_fare_mean[mean_not_survived]\n@not_survived_fare_std[std_not_survived]\n@fare_difference_significance[significance]\nwhere \"mean_survived\" is the mean fare of male passengers who survived, rounded to two decimal places.\nwhere \"std_survived\" is the standard deviation of fare of male passengers who survived, rounded to two decimal places.\nwhere \"mean_not_survived\" is the mean fare of male passengers who did not survive, rounded to two decimal places.\nwhere \"std_not_survived\" is the standard deviation of fare of male passengers who did not survive, rounded to two decimal places.\nwhere \"significance\" is a string that can either be \"significant\" or \"not significant\" based on the conditions specified in the constraints.", "file_name": "titanic_train.csv", "level": "hard", "answer": [["survived_fare_mean", "40.82"], ["not_survived_fare_std", "32.41"], ["fare_difference_significance", "significant"], ["not_survived_fare_mean", "21.96"], ["survived_fare_std", "71.36"]]}
{"id": 480, "question": "Apply feature engineering techniques to the dataset. Create a new feature by subtracting the mean value of the \"Value\" column from each value in that column. Calculate and report the standard deviation of this new feature.", "concepts": ["Feature Engineering", "Summary Statistics"], "constraints": "Create a new feature by subtracting the mean value of the \"Value\" column from each value in that column. Calculate the standard deviation of the new feature.", "format": "@standard_deviation[std_value] where \"std_value\" is a positive number rounded to two decimal places.", "file_name": "oecd_education_spending.csv", "level": "hard", "answer": [["standard_deviation", "1.22"]]}
{"id": 495, "question": "Perform outlier detection on the percentage of graduates in the field of Architecture over the years using the Z-score method with a threshold of 3. Identify all years with outliers, then calculate the mean and standard deviation for the years without these outliers.", "concepts": ["Outlier Detection", "Summary Statistics"], "constraints": "Use the Z-score method with a threshold of 3 for outlier detection.\nInclude all years in the dataset for the calculation.\nAfter identifying the outliers, remove them and then calculate the mean percentage and the standard deviation of the remaining data. Round to two decimal places.", "format": "@outlier_years[list of years with outliers]\n@mean_without_outliers[mean_value]\n@std_without_outliers[std_value]\nwhere \"list of years with outliers\" is a list of integer years in ascending order. \nwhere \"mean_value\" and \"std_value\" are floating point numbers rounded to two decimal places representing the mean and standard deviation, respectively.", "file_name": "percent-bachelors-degrees-women-usa.csv", "level": "hard", "answer": [["std_without_outliers", "9.57"], ["mean_without_outliers", "33.69"]]}
{"id": 496, "question": "Perform feature engineering by creating a new feature called \"STEM\" (Science, Technology, Engineering, and Math). It should be the sum of the percentages of graduates in the fields of Computer Science, Engineering, Math and Statistics, and Physical Sciences. Calculate the mean and range (maximum - minimum) of the \"STEM\" feature for the years beyond 2000.", "concepts": ["Feature Engineering", "Summary Statistics"], "constraints": "Calculate the new feature \"STEM\" as the sum of the percentages of graduates in the fields of Computer Science, Engineering, Math and Statistics, and Physical Sciences.\nCompute the mean and the range (maximum - minimum) of the \"STEM\" feature for the years 2000 and beyond. Round to two decimal places.", "format": "@mean_STEM[mean_value]\n@range_STEM[range_value]\nwhere \"mean_value\" is a floating point number rounded to two decimal places representing the mean of the \"STEM\" feature.\nwhere \"range_value\" is a floating point number rounded to two decimal places representing the range of the \"STEM\" feature.", "file_name": "percent-bachelors-degrees-women-usa.csv", "level": "hard", "answer": [["range_STEM", "17.7"], ["mean_STEM", "125.11"]]}
{"id": 572, "question": "Identify the date with the highest closing value of the S&P 500 Index (.SPX). Calculate the percentage change in the stock price of Apple Inc. (AAPL) from its closing price on the previous day to its closing price on the identified date.", "concepts": ["Summary Statistics", "Correlation Analysis"], "constraints": "1. The date where the S&P 500 Index (.SPX) reached its maximum value should be identified.\n2. The percentage change is calculated as: ((price on identified date / price on previous day) - 1) * 100.\n3. Percentage change should be calculated only if the previous day data exists. If the identified date is the first date in the dataset, state that the previous day data doesn't exist.\n4. The data for the previous day is defined as the data on the date immediately preceding the identified date when sorting the dates in ascending order. Hunting for the \"previous\" trading day is not required.", "format": "@max_SPX_date[date]\n@AAPL_price_percentage_change[percentage_change]\nwhere \"date\" is a string in the format YYYY-MM and \"percentage_change\" is a number rounded to two decimal places or the string \"Previous day data doesn't exist\".", "file_name": "tr_eikon_eod_data.csv", "level": "hard", "answer": [["max_SPX_date", "2018-01-26"], ["AAPL_price_percentage_change", "0.23"]]}
{"id": 574, "question": "Perform data preprocessing on the stock prices of Microsoft Corporation (MSFT), SPDR S&P 500 ETF Trust (SPY), and the CBOE Volatility Index (.VIX). This preprocessing includes removing missing values, normalizing the data, and encoding any categorical variables. Calculate the correlation matrix between the preprocessed stock prices.", "concepts": ["Comprehensive Data Preprocessing", "Correlation Analysis"], "constraints": "1. Missing values should be removed entirely from the dataset.\n2. The normalization method to be used is feature scaling (rescaling the data to range between 0 and 1).\n3. For categorical variables, use one hot encoding method, though no categorical data exists in the provided price columns.\n4. The correlation computation method to be used is Pearson's correlation.", "format": "@MSFT_SPY_correlation[correlation]\n@MSFT_VIX_correlation[correlation]\n@SPY_VIX_correlation[correlation]\nwhere \"correlation\" is a number between -1 and 1, rounded to two decimal places.", "file_name": "tr_eikon_eod_data.csv", "level": "hard", "answer": [["MSFT_VIX_correlation", "-0.43"], ["SPY_VIX_correlation", "-0.58"], ["MSFT_SPY_correlation", "0.94"]]}
{"id": 575, "question": "Using feature engineering techniques, create a new feature that represents the average stock price of Apple Inc. (AAPL), Microsoft Corporation (MSFT), and Amazon.com, Inc. (AMZN) on the given dates. Calculate the correlation between this new feature and the closing value of the S&P 500 Index (.SPX).", "concepts": ["Feature Engineering", "Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between the newly created average stock price feature and the closing value of the S&P 500 Index (.SPX).\nAssess the significance of the correlation using a two-tailed test with a significance level (alpha) of 0.05.\nReport the p-value associated with the correlation test.\nConsider the relationship to be linear if the p-value is less than 0.05 and the absolute value of r is greater than or equal to 0.5.\nConsider the relationship to be nonlinear if the p-value is less than 0.05 and the absolute value of r is less than 0.5.\nIf the p-value is greater than or equal to 0.05, report that there is no significant correlation.", "format": "@correlation_coefficient_corr[r_value]\n@p_value_pval[p_value]\n@relationship_type_relation[relationship_type]\nwhere \"r_value\" is a number between -1 and 1, rounded to two decimal places.\nwhere \"p_value\" is a number between 0 and 1, rounded to four decimal places.\nwhere \"relationship_type\" is a string that can either be \"linear\", \"nonlinear\", or \"none\" based on the conditions specified in the constraints.", "file_name": "tr_eikon_eod_data.csv", "level": "hard", "answer": [["relationship_type_relation", "linear"], ["p_value_pval", "0.0000"], ["correlation_coefficient_corr", "0.91"]]}
{"id": 590, "question": "Using machine learning techniques, can we predict the number of agents needed to handle incoming calls based on the timestamp and other available information? If so, predict the number for the timestamp \"20170413_120000\".", "concepts": ["Machine Learning"], "constraints": "Use a simple linear regression model for prediction. The model should be trained with features such as the timestamp, number of calls answered, number of call abandoned, etc., and the target variable should be the average number of agents staffed. Perform prediction for the given timestamp after training the model.", "format": "@predicted_agents[predicted_num_agents] where \"predicted_num_agents\" is a non-negative integer value representing the predicted number of agents for the specified timestamp.", "file_name": "20170413_000000_group_statistics.csv", "level": "hard", "answer": [["predicted_agents", "4"]]}
{"id": 593, "question": "Using feature engineering techniques, create a new feature that represents the waiting time for callers before being answered by an agent as a percentage of the average abandonment time. Then, explore the distribution of this new feature and determine if it adheres to a normal distribution.", "concepts": ["Feature Engineering", "Distribution Analysis"], "constraints": "Create a new feature 'waiting_ratio' that is defined as the ratio of average waiting time to the average abandonment time, represented as a percentage. Convert the waiting and abandonment time from format HH:MM:SS to seconds before the calculation. After creating the feature, calculate the skewness of this new feature. Use the skewness to determine whether the data is normally distributed. For normally distributed data, skewness should be about 0.", "format": "@waiting_ratio_skewness[skewness_value]\n@is_normal[is_normal]\nwhere \"skewness_value\" is the skewness of the 'waiting_ratio' feature rounded to two decimal places.\nwhere \"is_normal\" is a boolean value that should be \"True\" if the absolute value of skewness is less than 0.5 and \"False\" otherwise.", "file_name": "20170413_000000_group_statistics.csv", "level": "hard", "answer": [["is_normal", "False"]]}
{"id": 669, "question": "Identify and remove any outliers in the MedInc column of the provided dataset using the IQR method. Then calculate the mean and standard deviation of the cleaned MedInc column.", "concepts": ["Outlier Detection", "Summary Statistics"], "constraints": "Identify an outlier as any value that falls below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 are the first and third quartiles, respectively, and IQR is the interquartile range (Q3 - Q1). Calculate the mean and standard deviation to two decimal places.", "format": "@mean[mean_value] where \"mean_value\" is a float rounded to two decimal places. @standard_deviation[standard_deviation_value] where \"standard_deviation_value\" is a float rounded to two decimal places.", "file_name": "my_test_01.csv", "level": "hard", "answer": [["standard_deviation", "1.54"], ["mean", "3.73"]]}
{"id": 671, "question": "Build a machine learning model to predict the MedianHouseValue based on the following features:\n1. MedInc\n2. AveRooms\n3. Population\n4. Latitude\n5. Longitude\nSplit the dataset into training and testing sets, train the model using linear regression, and evaluate its performance using mean squared error (MSE).", "concepts": ["Machine Learning"], "constraints": "Split the dataset into 70% for training and 30% for testing. Use linear regression for the machine learning model. Calculate the MSE to three decimal places.", "format": "@mse[mse_value] where \"mse_value\" is a float rounded to three decimal places.", "file_name": "my_test_01.csv", "level": "hard", "answer": [["mse", "0.653"]]}
{"id": 673, "question": "Apply comprehensive data preprocessing on the dataset by following these steps:\n1. Replace any missing values in the MedInc column with the mean value.\n2. Standardize the values in the AveOccup column using z-scores.\n3. Create a new feature called \"RoomsPerPerson\" by dividing the AveRooms column by the Population column.\n4. Calculate the Pearson correlation coefficient between the MedianHouseValue and RoomsPerPerson columns.\n5. Finally, calculate the mean and standard deviation of the MedianHouseValue column.", "concepts": ["Comprehensive Data Preprocessing", "Feature Engineering", "Correlation Analysis", "Summary Statistics"], "constraints": "Use sklearn's StandardScaler for standardization. Use numpy to calculate the mean and standard deviation. Round all output to four decimal places.", "format": "@mean_value[mean_MedianHouseValue] \n@standard_deviation[stddev_MedianHouseValue] \n@pearson_coefficient[correlation_coefficient] where \"mean_MedianHouseValue\" and \"stddev_MedianHouseValue\" are floats representing the mean and standard deviation of the MedianHouseValue column rounded to four decimal places. \"correlation_coefficient\" is a float rounded to four decimal places, representing the correlation coefficient between the MedianHouseValue and RoomsPerPerson columns.", "file_name": "my_test_01.csv", "level": "hard", "answer": [["pearson_coefficient", "0.0382"], ["mean_value", "2.1226"]]}
{"id": 674, "question": "Build a machine learning model to predict the MedianHouseValue based on the following features:\n1. MedInc\n2. AveRooms\n3. HouseAge\n4. Latitude\n5. Longitude\nPerform the following steps:\n1. Split the dataset into training and testing sets, where 70% of the dataset is used for training and 30% for testing. Set the random_state as 42 for reproducibility.\n2. Preprocess the data by standardizing the numerical columns (MedInc, AveRooms, HouseAge, Latitude, Longitude).\n3. Train a decision tree regression model on the training set, setting the max_depth to 5.\n4. Evaluate the model's performance using mean absolute error (MAE) on the testing set.\n5. Finally, calculate the Pearson correlation coefficient between the predicted and actual MedianHouseValue values on the testing set.", "concepts": ["Machine Learning", "Comprehensive Data Preprocessing", "Correlation Analysis"], "constraints": "Use the sklearn library for splitting the dataset, preprocessing, training the model, and calculation of MAE. Set the random_state to 42 when splitting the dataset. Use the Pearson method to compute the correlation coefficient. Round all output to four decimal places.", "format": "@mean_absolute_error[mae_value] \n@pearson_coefficient[correlation_coefficient] where \"mae_value\" is a float representing the MAE on the testing set rounded to four decimal places, and \"correlation_coefficient\" is a float rounded to four decimal places representing the correlation coefficient between predicted and actual MedianHouseValue values on the testing set.", "file_name": "my_test_01.csv", "level": "hard", "answer": [["pearson_coefficient", "0.6419"], ["mean_absolute_error", "0.6426"]]}
{"id": 685, "question": "3. Is there a correlation between the atmospheric pressure and wind speed in the dataset?", "concepts": ["Correlation Analysis"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between atmospheric pressure and wind speed. Assess the significance of the correlation using a two-tailed test with a significance level (alpha) of 0.05. Report the p-value associated with the correlation test. Consider the relationship to be significant if the p-value is less than 0.05.", "format": "@correlation_coefficient[r_value] @p_value[value] @relationship_significance[significance], where \"r_value\" is a number between -1 and 1, rounded to two decimal places, \"value\" is a number representing the p-value from the correlation test, rounded to four decimal places, and \"significance\" is a string that can either be \"significant\" or \"not significant\" based on the p-value.", "file_name": "ravenna_250715.csv", "level": "hard", "answer": [["correlation_coefficient", "0.34"], ["relationship_significance", "not significant"], ["p_value", "0.1023"]]}
{"id": 690, "question": "2. Perform outlier detection on the wind speed column using Z-scores. Identify the number of outliers and provide the values of the outliers. After removing the outliers, calculate the mean and standard deviation of the wind speed column.", "concepts": ["Outlier Detection", "Summary Statistics"], "constraints": "Identify outliers using Z-score method considering points that have Z-score greater than 3 or less than -3 as outliers. After outlier detection, remove these identified outliers from the dataset and calculate the mean and standard deviation of the wind speed column.", "format": "@number_of_outliers[integer] @mean_wind_speed[number, rounded to 2 decimal places] @std_deviation_wind_speed[number, rounded to 2 decimal places]", "file_name": "ravenna_250715.csv", "level": "hard", "answer": [["mean_wind_speed", "2.29"], ["std_deviation_wind_speed", "1.15"], ["number_of_outliers", "0"]]}
{"id": 722, "question": "1. Identify the vehicle with the highest horsepower and provide its corresponding model year. Calculate the average horsepower along with the standard deviation for all vehicles within the same model year as this vehicle.", "concepts": ["Summary Statistics", "Comprehensive Data Preprocessing"], "constraints": "For the \"average horsepower\", calculate it using the arithmetic mean formula. Calculate the standard deviation using the population standard deviation formula, not the sample standard deviation formula. Round both measures to two decimal places.", "format": "@highest_horsepower_vehicle[vehicle_model_year]\n@average_horsepower[same_year_avg_horsepower]\n@standard_deviation[same_year_horsepower_std]\nwhere \"vehicle_model_year\" is an integer from 1900 to the current year. \"same_year_avg_horsepower\" and \"same_year_horsepower_std\" are numbers rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "hard", "answer": [["highest_horsepower_vehicle", "1973"], ["average_horsepower", "130.48"], ["standard_deviation", "45.83"]]}
{"id": 723, "question": "2. Generate a new feature called 'power-to-weight ratio' by dividing the horsepower by the weight for each vehicle. Calculate the mean and standard deviation of this new feature.", "concepts": ["Feature Engineering", "Summary Statistics"], "constraints": "Calculate the 'power-to-weight ratio' by dividing the horsepower by the weight for each vehicle, not the other way around. For the \"average power-to-weight ratio\", calculate it using the arithmetic mean formula. Calculate the standard deviation using the population standard deviation formula, not the sample standard deviation formula. Round both measures to two decimal places.", "format": "@mean_ratio[avg_power_weight_ratio]\n@std_ratio[power_weight_ratio_std]\nwhere \"avg_power_weight_ratio\" and \"power_weight_ratio_std\" are numbers rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "hard", "answer": [["mean_ratio", "0.03"], ["std_ratio", "0.01"]]}
{"id": 724, "question": "3. Perform outlier detection on the 'acceleration' column using the Z-score method. Identify any outliers and remove them from the dataset. Recalculate the mean and standard deviation of the 'acceleration' column after removing the outliers.", "concepts": ["Outlier Detection", "Summary Statistics", "Comprehensive Data Preprocessing"], "constraints": "Consider observations as outliers if their Z-scores are outside of the -3 to 3 range. For the \"average acceleration\" after outlier removal, calculate it using the arithmetic mean formula. Calculate the standard deviation using the population standard deviation formula, not the sample standard deviation formula. Round both measures to two decimal places.", "format": "@mean_acceleration[avg_acceleration]\n@std_acceleration[acceleration_std]\nwhere \"avg_acceleration\" and \"acceleration_std\" are numbers rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "hard", "answer": [["std_acceleration", "2.68"], ["mean_acceleration", "15.49"]]}
{"id": 725, "question": "1. Investigate the relationship between 'displacement' and 'mpg' by analyzing the distribution of 'mpg' for each unique value of 'displacement'. Calculate the mean and median 'mpg' for each of the three most common unique values of 'displacement'.", "concepts": ["Distribution Analysis", "Correlation Analysis"], "constraints": "{\n- Only consider the three unique 'displacement' values that occur most frequently in the dataset.\n- The 'mpg' means and medians must be calculated for each of these three values separately, with 'mpg' values only from rows with the corresponding 'displacement' value.\n- Results must be rounded to two decimal places.\n}", "format": "{\n@mean1[mean1], @median1[median1]\n@mean2[mean2], @median2[median2]\n@mean3[mean3], @median3[median3]\nwhere \"mean1\", \"median1\", \"mean2\", \"median2\", \"mean3\", \"median3\" are corresponding mean and median 'mpg' values for each of the top three 'displacement' values, respectively. Each value should be a float, rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "hard", "answer": [["median1", "28.0"], ["mean1", "28.73"]]}
{"id": 726, "question": "2. Perform comprehensive data preprocessing on the 'horsepower' column. Handle any missing values by imputing them with the mean horsepower value. Then, transform the 'horsepower' column by applying a log transformation. Calculate the mean and standard deviation of the transformed 'horsepower' column.", "concepts": ["Comprehensive Data Preprocessing", "Feature Engineering", "Summary Statistics"], "constraints": "{\n- Handle missing values by imputing them with the mean 'horsepower'.\n- Log-transformation should be a natural logarithm (base e).\n- Mean and standard deviation should be calculated after the transformation and rounding to two decimal places.\n}", "format": "{\n@mean_transformed_horsepower[mean_transformed_horsepower]\n@stddev_transformed_horsepower[stddev_transformed_horsepower]\nwhere \"mean_transformed_horsepower\" is the mean of the transformed 'horsepower' and \"stddev_transformed_horsepower\" is the standard deviation of the transformed 'horsepower'. Each value should be a float, rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "hard", "answer": [["mean_transformed_horsepower", "4.59"], ["stddev_transformed_horsepower", "0.34"]]}
{"id": 727, "question": "3. Use machine learning techniques to predict the 'mpg' of a vehicle based on its 'weight' and 'acceleration' features. Split the dataset into a training set and a testing set with the ratio of size 8:2. Train a linear regression model on the training set and evaluate its performance by calculating the mean squared error (MSE) on the testing set.", "concepts": ["Machine Learning", "Correlation Analysis"], "constraints": "{\n- Use the linear regression algorithm provided by the sklearn library in Python.\n- The dataset should be split into a training set and a testing set with the ratio 8:2 using a random_state of 42.\n- MSE should be calculated on the testing set only and rounding to two decimal places.\n}", "format": "{\n@test_mse[test_mse]\nwhere \"test_mse\" is the mean squared error of the testing set. The value should be a float, rounded to two decimal places.", "file_name": "auto-mpg.csv", "level": "hard", "answer": [["test_mse", "17.66"]]}
{"id": 732, "question": "Perform comprehensive data preprocessing for the dataset by handling missing values in the life expectancy column. Choose an appropriate strategy and implement it using Python code.", "concepts": ["Comprehensive Data Preprocessing"], "constraints": "Assume there are missing values in the life expectancy column.\nImpute missing values with the mean life expectancy of the same country.\nIf there are countries with all life expectancy values missing, replace missing values with the mean life expectancy of the entire dataset.", "format": "@number_of_missing_values_in_lifeexp_before[n_before]\n@number_of_missing_values_in_lifeexp_after[n_after]\nwhere \"n_before\" and \"n_after\" are integers representing the number of missing values in the life expectancy column before and after the imputation process.", "file_name": "gapminder_cleaned.csv", "level": "hard", "answer": [["number_of_missing_values_in_lifeexp_after", "0"]]}
{"id": 733, "question": "Apply feature engineering techniques to create a new feature in the dataset that represents the GDP per capita in logarithmic scale (base 10). Implement this feature transformation using Python code.", "concepts": ["Feature Engineering"], "constraints": "Calculate the logarithm with base 10.\nWhile calculating the logarithm, assume all GDP per capita figures are positive.", "format": "@has_nan_values_in_new_feature[boolean]\n@new_feature_mean[mean]\n@new_feature_std[std]\nwhere \"boolean\" is True or False, indicating whether there are NaN values in the newly created feature.\nwhere \"mean\" is a number (rounded to 2 decimal places) representing the mean of the newly created feature.\nwhere \"std\" is a number (rounded to 2 decimal places) representing the standard deviation of the newly created feature.", "file_name": "gapminder_cleaned.csv", "level": "hard", "answer": [["has_nan_values_in_new_feature", "False"], ["new_feature_mean", "3.54"], ["new_feature_std", "0.54"]]}
{"id": 734, "question": "Is there a correlation between life expectancy and GDP per capita for each continent? Perform correlation analysis for each continent separately and provide the correlation coefficients.", "concepts": ["Correlation Analysis", "Comprehensive Data Preprocessing"], "constraints": "Calculate the Pearson correlation coefficient (r) to assess the strength and direction of the linear relationship between life expectancy and GDP per capita for each continent. Assess the correlation significance using a two-tailed test with a significance level (alpha) of 0.05. Report the p-values associated with the correlation test. Consider the correlation significant if the p-value is less than 0.05 and the absolute value of r is greater than or equal to 0.5. Consider the correlation non-significant if the p-value is greater than or equal to 0.05.", "format": "For each continent:\n@continent_name[name]\n@correlation_coefficient[r_value]\n@p_value[p_value]\n@correlation_significance[significance]\nwhere \"name\" is the name of the continent.\nwhere \"r_value\" is a number between -1 and 1, rounded to two decimal places.\nwhere \"p_value\" is a number between 0 and 1, rounded to four decimal places.\nwhere \"significance\" is a string that can either be \"significant\" or \"non-significant\" based on the conditions specified in the constraints.", "file_name": "gapminder_cleaned.csv", "level": "hard", "answer": [["correlation_coefficient", "0.38"], ["correlation_significance", "significant"], ["correlation_significance", "non-significant"], ["correlation_coefficient", "0.78"], ["correlation_coefficient", "0.43"], ["correlation_coefficient", "0.96"], ["correlation_coefficient", "0.56"]]}
{"id": 736, "question": "Create a new feature by combining the population and GDP per capita columns. Normalize this new feature to a range of [0, 1]. Then, conduct a distribution analysis on this normalized feature and determine if it adheres to a normal distribution.", "concepts": ["Feature Engineering", "Distribution Analysis"], "constraints": "Define the new feature as population multiplied by GDP per capita. Normalize this new feature by applying min-max scaling. Perform a Shapiro-Wilk test to determine if the normalized feature follows a normal distribution. Consider the data to follow a normal distribution if the p-value of the Shapiro-Wilk test is greater than 0.05.", "format": "@is_normal[is_normal]\nwhere \"is_normal\" is a string that can be either \"yes\" or \"no\", indicating whether the normalized feature follows a normal distribution.", "file_name": "gapminder_cleaned.csv", "level": "hard", "answer": [["is_normal", "no"]]}
{"id": 759, "question": "5. Calculate the median and range of the maximum temperature (TMAX_F) for each type of observation (obs_type) recorded in the dataset. Are there any differences in the median and range between different observation types?", "concepts": ["Summary Statistics", "Distribution Analysis"], "constraints": "In your analysis:\n- Consider only two observation types: \"TMAX\" and \"TMIN\".\n- Report the median rounded to two decimal places.\n- Calculate the range as the difference between the maximum and minimum temperatures for each observation type.", "format": "@@median_tmax[\"median_TMAX\"]\n@median_tmin[\"median_TMIN\"]\n@range_tmax[\"range_TMAX\"]\n@range_tmin[\"range_TMIN\"]\n\nwhere \"median_TMAX\" and \"median_TMIN\" are the median temperatures for TMAX and TMIN observation types, respectively. They are numbers between -100 and 100, rounded to two decimal places.\nwhere \"range_TMAX\" and \"range_TMIN\" are the range of temperatures for TMAX and TMIN observation types, respectively. They are numbers greater than 0.", "file_name": "weather_data_1864.csv", "level": "hard", "answer": [["range_tmax", "125.82"], ["median_tmax", "58.64"]]}
